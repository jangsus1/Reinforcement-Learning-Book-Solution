\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{algpseudocode}
\usepackage[fleqn]{amsmath}

\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}

\title{Reinforcement Learning Solutions}
\author{Minsuk Chang}

\begin{document}
\maketitle
\pagebreak
\chapter{Introduction}

\section*{Exercise 1.1}
Probability will cause them to learn differently, because every movement they choose will be different by time to time.

\section*{Exercise 1.2}
Reducing the number of stages will be very beneficial, because number of stages will be reduced.

\section*{Exercise 1.3}
It max maximize temporal rewards, but will work poorly in long terms.

\section*{Exercise 1.4}
Exploratory moves make the model learn and converge fast to become optimal, but remains suboptimal because of the probability of exploration.

\section*{Exercise 1.5}
Learn from games which are already played by professional players.

\chapter{Multi Armed Bandits}
\section*{Exercise 2.1}
$P = (0.5*P(greedy))+(0.5*P(Random))$
$= (0.5*1)+(0.5*0.5)=0.75$

\section*{Exercise 2.2}
\begin{tabular}{ c | c c c c c }
 A & 1 & 2 & 2 & 2 & 3 \\
 \hline
 R & -1 & 1 & -2 & 2 & 0
\end{tabular}\\
Must have occurred: After 3, 4\\
Might have occurred : All intervals

\section*{Exercise 2.3}
$\epsilon=0.01$\\
If infinite time passes, 99\%of actions chosen will be optimal.

\section*{Exercise 2.4}

\chapter{Finite Markov Decision Processes}
\section*{Exercise 3.1}
Chess, Go, Tic-Tac-To

\section*{Exercise 3.2}
Games like slot machine which is  uniform random and each state isn't dependent on previous states.

\section*{Exercise 3.3}
1. Accelerator, steering wheel, and brake.\\
2. The boundary is drawn where the agent's absolute control reaches.\\
3. It may be free choice, but closer to the boundary, more efficient the learning process would be.

\section*{Exercise 3.4}

\section*{Exercise 3.5}
$S^+ \neq S$ in this notation.\\
$\sum_{s'\in S}\sum_{r \in R}p(s',r|s,a)=1$, for all $s\in S,a\in A(s)$\\\\
probability is 0 to move to other states from the terminal state.\\
$p(s',r|s,a)=0$, for all $s\in S,a\in A(s), s' \in S^+ - S$


\chapter{Dynamic Programming}

\section*{Exercise 4.4}
1. Initialization 
\begin{algorithmic}
\State $ V(s) \in R$ and $\pi(s) \in A(s)$ arbitrarily for all $s \in S; V(terminal)=0$
\end{algorithmic}
2. Policy Evaluation
\begin{algorithmic}
\State Loop:
\Indent
    \State $\Delta \leftarrow 0$
    \State Loop for each $s\in S:$
    \Indent
        \State $v \leftarrow V(s)$
        \State $V(s) \leftarrow \sum_{s',r}^{}p(s',r|s,\pi(s))[r+\gamma V(s')]$
        \State $\Delta \leftarrow max(\Delta,\left| v-V(s) \right|)$
    \EndIndent
    \State until $\Delta < \theta$(a small positive number determining the accuracy of estimation)
\EndIndent
\end{algorithmic}
3. Policy Improvement
\begin{algorithmic}
\State $policy$-$stable \leftarrow true$
\State For each $s \in S:$
\Indent
    \State $old$-$action \leftarrow \pi(s)$
    \State $\pi(s) \leftarrow argmax_a \sum_{s',r}p(s',r|s,a)[r+\gamma V(s')]$
    \State $maxval \leftarrow max_a \sum_{s',r}p(s',r|s,a)[r+\gamma V(s')]$
    \State If $old$-$action \neq \pi(s), maxval>V(s) $, then $policy$-$stable \leftarrow false$
\EndIndent
\State If $policy$-$stable$, then stop and return $V \approx v_* and \pi \approx \pi_*$; else go to 2
\end{algorithmic}

\section*{Exercise 4.5}
1. Initialization 
\begin{algorithmic}
\State $ Q(s,a) \in R$ and $\pi(s) \in A(s)$ arbitrarily for all $s \in S; Q(terminal, a)=0$ for all $a \in A(terminal)$
\end{algorithmic}
2. Policy Evaluation
\begin{algorithmic}
\State Loop:
\Indent
    \State $\Delta \leftarrow 0$
    \State Loop for each $s\in S:$
    \Indent
        \State Loop for each $a \in A(a):$
        \Indent
            \State $q \leftarrow Q(s, a)$
            \State $Q(s,a) \leftarrow \sum_{s',r}^{}p(s',r|s,\pi(s))[r+\gamma \sum_{a'}\pi(a'|s')Q(s',a')]$
            \State $\Delta \leftarrow max(\Delta,\left| q-Q(s,a) \right|)$
        \EndIndent
    \EndIndent
    \State until $\Delta < \theta$(a small positive number determining the accuracy of estimation)
\EndIndent
\end{algorithmic}
3. Policy Improvement
\begin{algorithmic}
\State $policy$-$stable \leftarrow true$
\State For each $s \in S:$
\Indent
    \State For each $a \in A(s):$
    \Indent
        \State $old$-$action \leftarrow \pi(s)$
        \State $\pi(s) \leftarrow argmax_a Q(s,a)$
        \State $maxval \leftarrow max_a Q(s,a)$
        \State If $old$-$action \neq \pi(s), maxval>Q(s, old$-$action) $, then $policy$-$stable \leftarrow false$
    \EndIndent 
\EndIndent
\State If $policy$-$stable$, then stop and return $Q \approx q_*$ and $\pi \approx \pi_*$; else go to 2
\end{algorithmic}

\section*{Exercise 4.6}
3 : The actions extracted by the policy would be depending on probability. This means that to improve the policy, we need to adjust the probability of actions to be selected on such state, but maintaining its minimum. When one of the action's probability reaches the minimum, then the algorithm needs to end.\\\\
2: To reach convergence, $\pi(s)$ needs to be replaced to $ argmax_a\pi(s)$\\\\
1: $\pi(s)$ is no longer an element but the probability of the actions of $A(s)$.

\end{document}
